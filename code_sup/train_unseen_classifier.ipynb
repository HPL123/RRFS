{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cls_models import ClsUnseenTrain\n",
    "from generate import load_seen_att\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from mmdetection.splits import get_seen_class_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %psource ClsUnseenTrain.forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = dotdict({\n",
    "    'dataset':'coco',\n",
    "    'classes_split': '65_15',\n",
    "    'class_embedding': 'MSCOCO/fasttext.npy',\n",
    "    'dataroot':'../../data/coco',\n",
    "    'trainsplit': 'train_0.6_0.3',\n",
    "    \n",
    "})\n",
    "# path to save the trained classifier best checkpoint\n",
    "path = 'MSCOCO/unseen_Classifier.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seen_att, att_labels = load_seen_att(opt)\n",
    "classid_tolabels = {l:i for i, l in enumerate(att_labels.data.numpy())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_classifier = ClsUnseenTrain(seen_att).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seen_features = np.load(f\"{opt.dataroot}/{opt.trainsplit}_feats.npy\")\n",
    "seen_labels = np.load(f\"{opt.dataroot}/{opt.trainsplit}_labels.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds = np.random.permutation(np.arange(len(seen_labels)))\n",
    "total_train_examples = int (0.8 * len(seen_labels))\n",
    "train_inds = inds[:total_train_examples]\n",
    "test_inds = inds[total_train_examples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_inds)+len(train_inds), len(seen_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feats = seen_features[train_inds]\n",
    "train_labels = seen_labels[train_inds]\n",
    "test_feats = seen_features[test_inds]\n",
    "test_labels = seen_labels[test_inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bg_inds = np.where(seen_labels==0)\n",
    "# fg_inds = np.where(seen_labels>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Featuresdataset(Dataset):\n",
    "     \n",
    "    def __init__(self, features, labels, classid_tolabels):\n",
    "        self.classid_tolabels = classid_tolabels\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_feature = self.features[idx]\n",
    "        batch_label = self.labels[idx]\n",
    "#         import pdb; pdb.set_trace()\n",
    "        \n",
    "        if self.classid_tolabels is not None:\n",
    "            batch_label = self.classid_tolabels[batch_label]\n",
    "        return batch_feature, batch_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seen_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_train = Featuresdataset(train_feats, train_labels, classid_tolabels)\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=512, shuffle=True) \n",
    "dataset_test = Featuresdataset(test_feats, test_labels, classid_tolabels)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=1024, shuffle=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.SGD(unseen_classifier.parameters(), lr=1, momentum=0.9)\n",
    "scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_val_loss = float(\"inf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val():\n",
    "    running_loss = 0.0\n",
    "    global min_val_loss\n",
    "    unseen_classifier.eval()\n",
    "    for i, (inputs, labels) in enumerate(dataloader_test, 0):\n",
    "        inputs = inputs.cuda()\n",
    "        labels = labels.cuda()\n",
    "        \n",
    "\n",
    "        outputs = unseen_classifier(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 200 == 199:\n",
    "            print(f'Test Loss {epoch + 1}, [{i + 1} / {len(dataloader_test)}], {(running_loss / i) :0.4f}')\n",
    "    if (running_loss / i) < min_val_loss:\n",
    "        min_val_loss = running_loss / i\n",
    "        state_dict = unseen_classifier.state_dict()   \n",
    "        torch.save(state_dict, path)\n",
    "        print(f'saved {min_val_loss :0.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for epoch in range(100):\n",
    "    unseen_classifier.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(dataloader_train, 0):\n",
    "        inputs = inputs.cuda()\n",
    "        labels = labels.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = unseen_classifier(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999: \n",
    "            print(f'Train Loss {epoch + 1}, [{i + 1} / {len(dataloader_train)}], {(running_loss / i) :0.4f}')\n",
    "    val()\n",
    "    scheduler.step()\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:zsd]",
   "language": "python",
   "name": "conda-env-zsd-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
